---
layout: post
date: 2024-12-16 14:15:18 -0500 # output of command `date +"%Y-%m-%d %H:%M:%S %z"`
title: "NeurIPS 2024: Forgetting/Unlearning is hard!"
mathjax: false
permalink: "/machine-learning/challenges-of-machine-unlearning.html"
categories: [Machine Learning, Unlearning, Forgetting, Neurips, Challenges of Machine Unlearning]
author: "Ali Faraji"
---

Recently, I attended the NeurIPS 2024 conference and encountered many fascinating papers and talks. Among these discussions, a recurring theme emerged: machine unlearning, particularly in the context of defining and quantifying unlearning.

To begin with, some papers argue that a certain degree of memorization is essential for models to generalize effectively [^onionMem] [^tale]. This is a compelling point, as memorization appears to play a critical role in enabling generalization. However, this raises an intriguing question: What happens if we attempt to unlearn that memorized data?

Consider a simple example that highlights the importance of memorization. While this example is straightforward and might not directly apply to the intricate architecture of a large language model or a complex neural network, it illustrates the concept well.

In a classifier like SVM (Support Vector Machine), the support vectors—data points close to the decision boundary—are crucial. We practically memorize these data to have the decision boundary [^svm]. If we remove these support vectors, the decision boundary shifts, and the model's ability to generalize diminishes significantly. This illustrates the idea that retaining certain memorized elements is essential for effective generalization.

{% include image.html url="/contents/svm_visualization.png" caption="Support Vector Machine (SVM) visualization with decision boundary and support vectors. Generated by [this code](/contents/svm_visualization.py)." %}

Additionally, knowledge itself is hierarchical and interconnected. For example, if I say I know linear algebra, it implies that I understand matrix multiplication, which in turn builds on a foundational understanding of scalar addition and multiplication. I cannot unlearn simple sum operations while retaining the ability to multiply matrices or the whole linear algebra knowledge.

This interconnected nature of knowledge leads to thought-provoking questions: Can I forget/change the sum operation while retaining more advanced concepts like matrix multiplication? Similarly, some types of knowledge, such as conclusions derived from a set of statements, are inherently complex. Can I unlearn the foundational statements while preserving the resulting conclusion?

These questions show that the unlearning is an interesting problem to work with and so many opportunities to have fundamental research.

-- Ali

## References

[^onionMem]: [The Privacy Onion Effect: Memorization is Relative](https://proceedings.neurips.cc/paper_files/paper/2022/file/564b5f8289ba846ebc498417e834c253-Paper-Conference.pdf)

[^tale]: [Does Learning Require Memorization? A Short Tale about a Long Tail](https://arxiv.org/abs/1906.05271)

[^svm]: [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine)